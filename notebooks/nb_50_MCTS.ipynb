{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "hide_input": false,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv",
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_ml_control\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Protocol\n",
    "\n",
    "import casadi\n",
    "import do_mpc\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from gymnasium import Env\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from training_ml_control.environment import (\n",
    "    create_inverted_pendulum_environment,\n",
    ")\n",
    "\n",
    "from training_ml_control.shortest_path_problem import (\n",
    "    create_shortest_path_graph,\n",
    "    plot_shortest_path_graph,\n",
    "    plot_all_paths_graph,\n",
    ")\n",
    "\n",
    "from training_ml_control.plots import (\n",
    "    create_inverted_pendulum_environment,\n",
    "    plot_inverted_pendulum_results,\n",
    "    animate_mass_spring_damper_simulation,\n",
    "    animate_inverted_pendulum_simulation,\n",
    "    animate_full_inverted_pendulum_simulation,\n",
    ")\n",
    "\n",
    "from training_ml_control.nb_utils import (\n",
    "    display_array,\n",
    "    show_video,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "sns.set_theme()\n",
    "plt.rcParams[\"figure.figsize\"] = [9, 5]\n",
    "# This is needed because inside docker the rendering of mujoco environments may not work.\n",
    "render_mode = \"rgb_array\" if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    ":::{figure} ./_static/images/aai-institute-cover.png\n",
    ":width: 90%\n",
    ":align: center\n",
    "---\n",
    "name: aai-institute\n",
    "---\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unfortunately, the analytically convenient linear quadratic problem formulations are often not satisfactory. There are two main reasons for this:\n",
    "\n",
    "- The system may be nonlinear, and it may be inappropriate to use for\n",
    "  control purposes. Moreover, some of the control variables may be naturally\n",
    "  discrete, and this is incompatible with the linear system viewpoint.\n",
    "  \n",
    "- There may be control and/or state constraints, which are not handled\n",
    "  adequately through quadratic penalty terms in the cost function. For\n",
    "  example, the motion of a car may be constrained by the presence of\n",
    "  obstacles and hardware limitations.\n",
    "  The solution obtained from a linear quadratic model may not be suitable for such\n",
    "  a problem, because quadratic penalties treat constraints \"softly\"\n",
    "  and may produce trajectories that violate the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Model Predictive Control (MPC) is a control algorithm based on solving an **on-line** optimal control problem. A receding horizon approach is used which can be summarized in the following steps:\n",
    "\n",
    "1. At time $t$ and for the current state $x_t$, solve, on-line, an open-loop optimal control problem over some future interval taking account the current and future constraints.\n",
    "2. Apply the first step in the optimal control sequence.\n",
    "3. Repeat the procedure at time $t + 1$ using the current state $x_{t + 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{figure} _static/images/30_model_predictive_control_horizon.svg\n",
    "Illustration of the problem solved by MPC at state $x_k$.\n",
    "We minimize the cost function over the next $l$ stages.\n",
    "We then apply the control of the optimizing sequence up to the control horizon.\n",
    "In most cases, the control horizon is set to 1.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    ":::{figure} _static/images/30_mpc_block_diagram.svg\n",
    "MPC Block Diagram.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A major difference between MPC and finite-state stochastic control problems\n",
    "that are popular in the RL/artificial intelligence literature is that in MPC\n",
    "the state and control spaces are continuous/infinite, such as for example\n",
    "in self-driving cars, the control of aircraft and drones, or the operation of\n",
    "chemical processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iterative Linear Quadratic Regulator\n",
    "\n",
    "Iterative Linear Quadratic Regulator (iLQR) is an extension of LQR control to non-linear system with non-linear quadratic costs.\n",
    "\n",
    "The idea is to approximate the cost and dynamics as quadratic and affine, respectively, then exactly solve the resulting LQR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a discrete-time non-linear system with state-space representation: \n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t)\n",
    "$$\n",
    "\n",
    "we approximate it with a first-order Taylor-series expansion about nominal trajectories\n",
    "$\\mathbf{X} = \\{\\mathbf{x}_0, \\dots, \\mathbf{x}_N\\}, \\mathbf{U} = \\{ \\mathbf{u}_0, \\dots, \\mathbf{u}_N \\}$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mathbf{x}_{t+1} + \\delta\\mathbf{x}_{t+1} &= f(\\mathbf{x}_t + \\delta \\mathbf{x}_t, \\mathbf{u}_t + \\delta \\mathbf{u}_t) \\\\ \n",
    "& \\approx f(\\mathbf{x}_t, \\mathbf{u}_t)\n",
    "+ \\frac{\\partial f}{\\partial \\mathbf{x}}|_{\\mathbf{x}_t, \\mathbf{u}_t}\\delta\\mathbf{x}_t\n",
    "+ \\frac{\\partial f}{\\partial \\mathbf{u}}|_{\\mathbf{x}_t, \\mathbf{u}_t}\\delta\\mathbf{u}_t\\\\\n",
    "\\delta\\mathbf{x}_{t+1} &= A_t\\delta\\mathbf{x}_t + B_t\\delta\\mathbf{u}_t\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "with\n",
    "$A_t = A(\\mathbf{x}_t, \\mathbf{u}_t) = \\frac{\\partial f}{\\partial \\mathbf{x}}|_{\\mathbf{x}_t, \\mathbf{u}_t}$ and\n",
    "$B_t = B(\\mathbf{x}_t, \\mathbf{u}_t) = \\frac{\\partial f}{\\partial \\mathbf{u}}|_{\\mathbf{x}_t, \\mathbf{u}_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a general cost function that is not linear-quadratic, we use a second-order Taylor Series Expansion to linearize the dynamics into a form common for optimal control problems:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "J_N(\\mathbf{x}_0, \\mathbf{U}) &=\n",
    "l_f(\\mathbf{x}_N) + \\sum \\limits_{t=1}^{N-1} l(\\mathbf{x}_t, \\mathbf{u}_t)\\\\\n",
    "&\\approx \\frac{1}{2} \\mathbf{x}^T_N Q \\mathbf{x}_N + q_N^T \\mathbf{x}_N\n",
    "+ \\sum \\limits_{t=1}^{N-1}\n",
    "\\frac{1}{2} \\mathbf{x}^T_t Q_t \\mathbf{x}_t\n",
    "+ \\frac{1}{2} \\mathbf{u}^T_t R_t \\mathbf{u}_t\n",
    "+ \\frac{1}{2} \\mathbf{x}^T_t H_t \\mathbf{u}_t\n",
    "+ \\frac{1}{2} \\mathbf{u}^T_t H_t^T \\mathbf{x}_t\n",
    "+ q_t^T \\mathbf{x}_t +  + r_t^T \\mathbf{u}_t\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We define the following variables for convenience:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "l_x &:= \\frac{\\partial l}{\\partial \\mathbf{x}}|_{\\mathbf{x}_t, \\mathbf{u}_t} = Q_t \\mathbf{x}_t + q_t\n",
    "\\\\\n",
    "l_u &:= \\frac{\\partial l}{\\partial \\mathbf{u}}|_{\\mathbf{x}_t, \\mathbf{u}_t} = R_t \\mathbf{u}_t + r_t\n",
    "\\\\\n",
    "l_{xx} &:= \\frac{\\partial^2 l}{\\partial \\mathbf{x}^2}|_{\\mathbf{x}_t, \\mathbf{u}_t} = Q_t\n",
    "\\\\\n",
    "l_{uu} &:= \\frac{\\partial^2 l}{\\partial \\mathbf{u}^2}|_{\\mathbf{x}_t, \\mathbf{u}_t} = R_t\n",
    "\\\\\n",
    "l_{xu} &:= \\frac{\\partial^2 l}{\\partial \\mathbf{x}\\mathbf{u}}|_{\\mathbf{x}_t, \\mathbf{u}_t} = H_t\n",
    "\\\\\n",
    "l_{ux} &:= \\frac{\\partial^2 l}{\\partial \\mathbf{u}\\mathbf{x}}|_{\\mathbf{x}_t, \\mathbf{u}_t} = H_t^T\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can apply Bellman's Principle of Optimality to define the optimal cost-to-go:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "V_N(\\mathbf{x}_N) &= g_f(\\mathbf{x}_N)\\\\\n",
    "V_{t}(\\mathbf{x}_t) &= \\displaystyle \\min_u {g(\\mathbf{x}_{t}, \\mathbf{u}_{t}) + V_{t+1}(f(\\mathbf{x}_{t}, \\mathbf{u}_{N-t}))}\\\\\n",
    "V_{t}(\\mathbf{x}_t) &= \\displaystyle \\min_u Q_{t}(\\mathbf{x}_{t}, \\mathbf{u}_{t})\n",
    "\\end{array}\\\\\n",
    "$$\n",
    "\n",
    "The $Q$-function is the discrete-time analogue of the Hamiltonian, sometimes known as the pseudo-Hamiltonian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We approximate the cost-to-go function as locally quadratic near the nominal trajectory gives us (we drop the time index in some of the equations for the sake of readability):\n",
    "\n",
    "$$\n",
    "\\delta V(\\mathbf{x}) = s^T \\delta\\mathbf{x} + \\frac{1}{2} \\delta\\mathbf{x}^T S \\delta\\mathbf{x}\n",
    "$$\n",
    "\n",
    "with \n",
    "$s = \\frac{\\partial V}{\\partial \\mathbf{x}}|_{\\mathbf{x}},\n",
    "S = \\frac{\\partial^2 V}{\\partial \\mathbf{x}^2}|_{\\mathbf{x}}$\n",
    "\n",
    "Similarily:\n",
    "\n",
    "$$\n",
    "\\delta Q(\\mathbf{x}, \\mathbf{u}) = \n",
    "\\frac{1}{2}\n",
    "\\begin{bmatrix} \\delta\\mathbf{x} \\\\ \\delta\\mathbf{u} \\end{bmatrix}^T\n",
    "\\begin{bmatrix} Q_{xx} & Q_{xu} \\\\ Q_{ux} & Q_{uu} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\delta\\mathbf{x} \\\\ \\delta\\mathbf{u} \\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix} Q_{x} \\\\ Q_{u} \\end{bmatrix}^T\n",
    "\\begin{bmatrix} \\delta\\mathbf{x} \\\\ \\delta\\mathbf{u} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "Q_x &= l_x + s_{t+1} A_t \\\\\n",
    "Q_u &= l_u + s_{t+1} B_t \\\\\n",
    "Q_{xx} &= l_{xx} + A_t^T S_{t+1} A_t \\\\\n",
    "Q_{uu} &= l_{uu} + B_t^T S_{t+1} B_t \\\\\n",
    "Q_{ux} &= l_{ux} + B_t^T S_{t+1} A_t\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The optimal control modification $\\delta\\mathbf{u}^∗$ for some state perturbation $\\delta\\mathbf{x}$, is obtained by minimizing the quadratic model:\n",
    "\n",
    "$$\n",
    "\\delta\\mathbf{u}^∗(\\delta\\mathbf{x}) = \\displaystyle\\arg\\min_{\\delta\\mathbf{u}} Q(\\delta\\mathbf{x}, \\delta\\mathbf{u}) = k + K\\delta\\mathbf{x}\n",
    "$$\n",
    "\n",
    "This is a locally-linear feedback policy with:\n",
    "\n",
    "$$\n",
    "k := -Q_{uu}^{-1}Q_u\\\\\n",
    "K := -Q_{uu}^{-1}Q_{ux}\n",
    "$$\n",
    "\n",
    "Plugging this back into the expansion of $Q$, a quadratic model of $V$ is obtained. After simplification it is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\Delta V &= \\frac{1}{2} k^T Q_{uu} k + k^T Q_u\\\\\n",
    "s &= Q_x + K^T Q_{uu} k + K^T Q_u + Q_{ux}^T k\\\\\n",
    "S &= Q_{xx} + K^T Q_{uu} K + K^T Q_{ux} + Q_{ux}^T K\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once these terms are computed, a forward pass computes a new trajectory:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\hat{x}_0 &= x_0\\\\\n",
    "\\hat{u}_t &= u_t + k_t + K_t(\\hat{x}_t - x_t)\\\\\n",
    "\\hat{x}_{t+1} &= f(\\hat{x}_t, \\hat{u}_t)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The basic flow of the algorithm is:\n",
    "\n",
    "1. Initialize with initial state $\\mathbf{x}_0$ and initial control sequence $\\mathbf{U} = \\{\\mathbf{u}_{0}, \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{N-1}\\}$.\n",
    "2. Do a forward pass using the non-linear dynamics, i.e. simulate the system using $(\\mathbf{x}_0, \\mathbf{U})$ to get the trajectory through state space, $\\mathbf{X}$, that results from applying the control sequence $\\mathbf{X}$ starting in $\\mathbf{x}_0$.\n",
    "3. Do a backward pass, estimate the value function and dynamics for each $(\\mathbf{x}, \\mathbf{u})$ in the state-space and control signal trajectories.\n",
    "4. Calculate an updated control signal $\\hat{\\mathbf{U}}$ and evaluate cost of trajectory resulting from $(\\mathbf{x}_0, \\hat{\\mathbf{U}})$.\n",
    "   \n",
    "   1. If $|(\\textrm{cost}(x_0, \\hat{\\textbf{U}}) - \\textrm{cost}(x_0, \\textbf{U})| < \\textrm{threshold},$ then we've converged and exit.\n",
    "   2. If $\\textrm{cost}(x_0, \\hat{\\textbf{U}}) < \\textrm{cost}(x_0, \\textbf{U}),$ then set $\\textbf{U} = \\hat{\\textbf{U}},$ and change the update size to be more aggressive. Go back to step 2.\n",
    "   3. If $\\textrm{cost}(x_0, \\hat{\\textbf{U}}) \\geq \\textrm{cost}(x_0, \\textbf{U}),$ change the update size to be more modest. Go back to step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Differential Dynamic Programming\n",
    "\n",
    "Differential Dynamic Programming (DDP) is almost identical to iLQR but unlike the latter, it expands the system's dynamics to the second order i.e. for a given a discrete-time non-linear system with state-space representation: \n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t)\n",
    "$$\n",
    "\n",
    "we approximate it with a **second-order** Taylor-series expansion about nominal trajectories\n",
    "$\\mathbf{X} = \\{\\mathbf{x}_0, \\dots, \\mathbf{x}_N\\}, \\mathbf{U} = \\{ \\mathbf{u}_0, \\dots, \\mathbf{u}_N \\}$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mathbf{x}_{t+1} + \\delta\\mathbf{x}_{t+1} &= f(\\mathbf{x}_t + \\delta \\mathbf{x}_t, \\mathbf{u}_t + \\delta \\mathbf{u}_t) \\\\ \n",
    "& \\approx f(\\mathbf{x}_t, \\mathbf{u}_t)\n",
    "+ \\frac{\\partial f}{\\partial \\mathbf{x}}|_{\\mathbf{x}_t, \\mathbf{u}_t}\\delta\\mathbf{x}_t\n",
    "+ \\frac{\\partial f}{\\partial \\mathbf{u}}|_{\\mathbf{x}_t, \\mathbf{u}_t}\\delta\\mathbf{u}_t\n",
    "+ \\frac{1}{2} \\left(\n",
    "\\frac{\\partial^2 f}{\\partial \\mathbf{x}^2}|_{\\mathbf{x}_t, \\mathbf{u}_t}\\delta\\mathbf{x}_t^2\n",
    "+ 2 \\frac{\\partial^2 f}{\\partial \\mathbf{x}\\mathbf{u}}|_{\\mathbf{x}_t, \\mathbf{u}_t}\\delta\\mathbf{x}_t\\delta\\mathbf{u}_t\n",
    "+ \\frac{\\partial^2 f}{\\partial \\mathbf{u}^2}|_{\\mathbf{x}_t, \\mathbf{u}_t}\\delta\\mathbf{u}_t^2\n",
    "\\right)\n",
    "\\\\\n",
    "\\delta\\mathbf{x}_{t+1} &= f_x\\delta\\mathbf{x}_t + f_u\\delta\\mathbf{u}_t + \\frac{1}{2} \\left(\n",
    "f_{xx} \\delta\\mathbf{x}_t^2 + 2 f_{xu} \\delta\\mathbf{x}_t \\delta\\mathbf{u}_t + f_{uu} \\delta\\mathbf{u}_t^2\n",
    "\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Which leads to a different expansion of the $Q$-function:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "Q_x &= l_x + f_x^T s_{t+1} \\\\\n",
    "Q_u &= l_u + f_u^T s_{t+1} \\\\\n",
    "Q_{xx} &= l_{xx} + f_x^T S_{t+1} f_x + S_{t+1} \\cdot f_{xx} \\\\\n",
    "Q_{uu} &= l_{uu} + B_t^T S_{t+1} B_t + S_{t+1} \\cdot f_{xu} \\\\\n",
    "Q_{ux} &= l_{ux} + B_t^T S_{t+1} A_t + S_{t+1} \\cdot f_{uu} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The difference with iLQR is that the last term in the last 3 equations is the product of a vector with a tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The optimal control modification $\\delta\\mathbf{u}^∗$ for some state perturbation $\\delta\\mathbf{x}$, is obtained by minimizing the quadratic model:\n",
    "\n",
    "$$\n",
    "\\delta\\mathbf{u}^∗(\\delta\\mathbf{x}) = \\displaystyle\\arg\\min_{\\delta\\mathbf{u}} Q(\\delta\\mathbf{x}, \\delta\\mathbf{u}) = k + K\\delta\\mathbf{x}\n",
    "$$\n",
    "\n",
    "This is a locally-linear feedback policy with:\n",
    "\n",
    "$$\n",
    "k := -Q_{uu}^{-1}Q_u\\\\\n",
    "K := -Q_{uu}^{-1}Q_{ux}\n",
    "$$\n",
    "\n",
    "Plugging this back into the expansion of $Q$, a quadratic model of $V$ is obtained. After simplification it is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\Delta V &= -\\frac{1}{2} k^T Q_{uu} k\\\\\n",
    "s &= Q_x - K^T Q_{uu} k\\\\\n",
    "S &= Q_{xx} - K^T Q_{uu} K\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mass-Spring-Damper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[do-mpc](https://www.do-mpc.com/en/latest/) uses [CasADi](https://web.casadi.org/python-api/) (an open-source tool for nonlinear optimization and algorithmic differentiation) for the modeling part and for the different cost functions. Here's a table of useful operators:\n",
    "\n",
    "| Operator | Description |Equation |\n",
    "| --- | --- | --- |\n",
    "| [casadi.sumsqr(x)](https://web.casadi.org/python-api/#casadi.casadi.sumsqr) | Squared-sum | $\\sum x_i^2$ |\n",
    "| [casadi.norm_2(x)](https://web.casadi.org/python-api/#casadi.casadi.norm_2) | $L_2$-norm | $\\sqrt{\\sum x_i^2}$ |\n",
    "| [casadi.norm_1(x)](https://web.casadi.org/python-api/#casadi.casadi.norm_1) | $L_1$-norm | $\\sum |x_i|$ |\n",
    "| [casadi.bilin(A, x)](https://web.casadi.org/python-api/#casadi.casadi.bilin) | Quadratic Form | $x^T A x$ |\n",
    "| [casadi.bilin(A, x, y)](https://web.casadi.org/python-api/#casadi.casadi.bilin) | Bilinear Form | $x^T A y$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Controller\n",
    "\n",
    "First, we create an instance of the MPC class is generated with the Mass-Spring-Damper prediction model defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mpc = do_mpc.controller.MPC(mass_spring_damper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We choose the finite prediction horizon `n_horizon = 20`, the time step `t_step = 0.04s` to be the same as the environment's time step. We also set the parameters of the applied discretization scheme orthogonal collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = create_mass_spring_damper_environment()\n",
    "mpc_params = {\n",
    "    \"n_horizon\": 20,\n",
    "    \"t_step\": env.dt,\n",
    "    \"state_discretization\": \"collocation\",\n",
    "    \"collocation_type\": \"radau\",\n",
    "    \"collocation_deg\": 3,\n",
    "    \"collocation_ni\": 1,\n",
    "    \"store_full_solution\": True,\n",
    "    # Use MA27 linear solver in ipopt for faster calculations:\n",
    "    \"nlpsol_opts\": {\"ipopt.linear_solver\": \"mumps\"},\n",
    "}\n",
    "mpc.set_param(**mpc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Objective\n",
    "\n",
    "The control objective is to move the mass to a desired position (`0.1`) and keep it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xss = np.array([0.1, 0.0])\n",
    "distance_cost = 100 * casadi.norm_2(mass_spring_damper.x.cat - xss)\n",
    "terminal_cost = distance_cost\n",
    "stage_cost = distance_cost\n",
    "print(f\"{stage_cost=}\")\n",
    "print(f\"{terminal_cost=}\")\n",
    "mpc.set_objective(mterm=terminal_cost, lterm=stage_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also restrict the input force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "force_penalty = 1e-2\n",
    "mpc.set_rterm(force=force_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Constraints\n",
    "\n",
    "We apply constraints on the force. In this case, there is only an upper and lower bounds for the force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# lower and upper bounds of the input\n",
    "u_max = 20\n",
    "mpc.bounds[\"lower\", \"_u\", \"force\"] = -u_max\n",
    "mpc.bounds[\"upper\", \"_u\", \"force\"] = u_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setup\n",
    "\n",
    "We can now setup the controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mpc.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Simulation\n",
    "\n",
    "We set the initial state and simulate the closed-loop for 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "mpc.reset_history()\n",
    "mass_spring_damper_simulator.reset_history()\n",
    "x0 = np.zeros((2, 1))\n",
    "mass_spring_damper_simulator.x0 = x0\n",
    "mpc.x0 = x0\n",
    "mpc.set_initial_guess()\n",
    "for k in range(100):\n",
    "    u0 = mpc.make_step(x0)\n",
    "    x0 = mass_spring_damper_simulator.make_step(u0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "animate_mass_spring_damper_simulation(mpc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "Finally, we evaluate the controller on the actual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MPCController:\n",
    "    def __init__(self, mpc: do_mpc.controller.MPC) -> None:\n",
    "        self.mpc = mpc\n",
    "        self.mpc.reset_history()\n",
    "        self.mpc.x0 = np.zeros(2)\n",
    "        self.mpc.set_initial_guess()\n",
    "\n",
    "    def act(self, observation: NDArray) -> NDArray:\n",
    "        return mpc.make_step(observation.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "max_steps = 100\n",
    "env = create_mass_spring_damper_environment(\n",
    "    render_mode=render_mode, max_steps=max_steps\n",
    ")\n",
    "controller = MPCController(mpc)\n",
    "results = simulate_environment(env, max_steps=max_steps, controller=controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show_video(results.frames, fps=1 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "animate_mass_spring_damper_simulation(mpc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "- Design an MPC controller for the linearized inverted pendulum.\n",
    "- For each case, try different cost functions:\n",
    "  - $\\sum \\theta^2$\n",
    "  - $\\sum |\\theta|$\n",
    "  - $E_{\\text{kinetic}} - E_{\\text{potential}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mpc = do_mpc.controller.MPC(inverted_pendulum_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "env = create_inverted_pendulum_environment()\n",
    "mpc_params = {\n",
    "    \"n_horizon\": 50,\n",
    "    \"t_step\": env.dt,\n",
    "    \"state_discretization\": \"collocation\",\n",
    "    \"collocation_type\": \"radau\",\n",
    "    \"collocation_deg\": 3,\n",
    "    \"collocation_ni\": 1,\n",
    "    \"store_full_solution\": True,\n",
    "    # Use MA27 linear solver in ipopt for faster calculations:\n",
    "    \"nlpsol_opts\": {\"ipopt.linear_solver\": \"mumps\"},\n",
    "}\n",
    "mpc.set_param(**mpc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "xss = np.array([0.0, 0.0])\n",
    "distance_cost = casadi.bilin(np.diag([100, 1]), inverted_pendulum_lin.x.cat - xss)\n",
    "terminal_cost = distance_cost\n",
    "stage_cost = distance_cost\n",
    "print(f\"{stage_cost=}\")\n",
    "print(f\"{terminal_cost=}\")\n",
    "mpc.set_objective(mterm=terminal_cost, lterm=stage_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "force_penalty = 1e-4\n",
    "mpc.set_rterm(force=force_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# lower and upper bounds of the input\n",
    "u_max = 3\n",
    "mpc.bounds[\"lower\", \"_u\", \"force\"] = -u_max\n",
    "mpc.bounds[\"upper\", \"_u\", \"force\"] = u_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mpc.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "mpc.reset_history()\n",
    "inverted_pendulum_lin_simulator.reset_history()\n",
    "x0 = np.zeros((2, 1))\n",
    "x0[0] = 0.01\n",
    "inverted_pendulum_lin_simulator.x0 = x0\n",
    "mpc.x0 = x0\n",
    "mpc.set_initial_guess()\n",
    "for k in range(100):\n",
    "    u0 = mpc.make_step(x0)\n",
    "    x0 = inverted_pendulum_lin_simulator.make_step(u0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "animate_inverted_pendulum_simulation(mpc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class MPCController:\n",
    "    def __init__(self, mpc: do_mpc.controller.MPC) -> None:\n",
    "        self.mpc = mpc\n",
    "        self.mpc.reset_history()\n",
    "        self.mpc.x0 = np.zeros(2)\n",
    "        self.mpc.set_initial_guess()\n",
    "\n",
    "    def act(self, observation: NDArray) -> NDArray:\n",
    "        return mpc.make_step(observation[[1, 3]].reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "max_steps = 500\n",
    "env = create_inverted_pendulum_environment(render_mode=render_mode, max_steps=max_steps)\n",
    "controller = MPCController(mpc)\n",
    "results = simulate_environment(env, max_steps=max_steps, controller=controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "show_video(results.frames, fps=1 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "animate_inverted_pendulum_simulation(mpc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "- Design an MPC Controller for the non-linear inverted pendulum system for two different cases:\n",
    "  1. Cart at origin and upright Pendulm: Set the reference for the cart position to the origin.\n",
    "  2. Pendulum Swing-up: Set the initial angle to to $-\\pi$ i.e. start with the pendulum at the bottom.\n",
    "  3. Same as 2 but set the reference for the cart position to the origin.\n",
    "  4. Make the pendulum rotate as fast as possible.\n",
    "  \n",
    "> **Note 1** Use the following to create the environment with initial angle set to -np.pi and cutoff angle to np.inf for second > case.\n",
    "> ```python\n",
    "> env = create_inverted_pendulum_environment(cutoff_angle=np.inf, initial_angle=-np.pi)\n",
    "> ```\n",
    "\n",
    "> **Note 2**: You can access the inverted pendulum's kinetic, respectively potential, energy using\n",
    "> `inverted_pendulum.aux[\"E_kinetic\"]`, respectively `inverted_pendulum.aux[\"E_potential\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Swing-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mpc = do_mpc.controller.MPC(inverted_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "env = create_inverted_pendulum_environment()\n",
    "mpc_params = {\n",
    "    \"n_horizon\": 100,\n",
    "    \"t_step\": env.dt,\n",
    "    \"state_discretization\": \"collocation\",\n",
    "    \"collocation_type\": \"radau\",\n",
    "    \"collocation_deg\": 3,\n",
    "    \"collocation_ni\": 1,\n",
    "    \"store_full_solution\": True,\n",
    "    # Use MA27 linear solver in ipopt for faster calculations:\n",
    "    \"nlpsol_opts\": {\"ipopt.linear_solver\": \"mumps\"},\n",
    "}\n",
    "mpc.set_param(**mpc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "energy_cost = inverted_pendulum.aux[\"E_kinetic\"] - inverted_pendulum.aux[\"E_potential\"]\n",
    "terminal_cost = energy_cost\n",
    "stage_cost = energy_cost\n",
    "print(f\"{stage_cost=}\")\n",
    "print(f\"{terminal_cost=}\")\n",
    "mpc.set_objective(mterm=terminal_cost, lterm=stage_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "force_penalty = 0.1\n",
    "mpc.set_rterm(force=force_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# lower and upper bounds of the position\n",
    "x_max = 1\n",
    "mpc.bounds[\"lower\", \"_x\", \"position\"] = -x_max\n",
    "mpc.bounds[\"upper\", \"_x\", \"position\"] = x_max\n",
    "# lower and upper bounds of the input\n",
    "u_max = 3\n",
    "mpc.bounds[\"lower\", \"_u\", \"force\"] = -u_max\n",
    "mpc.bounds[\"upper\", \"_u\", \"force\"] = u_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mpc.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "mpc.reset_history()\n",
    "inverted_pendulum_simulator.reset_history()\n",
    "x0 = np.array([0.0, -0.99 * np.pi, 0.0, 0.0])\n",
    "inverted_pendulum_simulator.x0 = x0\n",
    "mpc.x0 = x0\n",
    "mpc.set_initial_guess()\n",
    "for k in range(100):\n",
    "    u0 = mpc.make_step(x0)\n",
    "    x0 = inverted_pendulum_simulator.make_step(u0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "animate_full_inverted_pendulum_simulation(mpc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class MPCController:\n",
    "    def __init__(self, mpc: do_mpc.controller.MPC) -> None:\n",
    "        self.mpc = mpc\n",
    "        self.mpc.reset_history()\n",
    "        self.mpc.x0 = np.array([0.0, -0.99 * np.pi, 0.0, 0.0])\n",
    "        self.mpc.set_initial_guess()\n",
    "\n",
    "    def act(self, observation: NDArray) -> NDArray:\n",
    "        return mpc.make_step(observation.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "max_steps = 500\n",
    "env = create_inverted_pendulum_environment(\n",
    "    render_mode=render_mode,\n",
    "    max_steps=max_steps,\n",
    "    cutoff_angle=np.inf,\n",
    "    initial_angle=-np.pi,\n",
    ")\n",
    "controller = MPCController(mpc)\n",
    "results = simulate_environment(env, max_steps=max_steps, controller=controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "show_video(results.frames, fps=1 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "animate_full_inverted_pendulum_simulation(mpc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# MPC and AlphaZero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.\n",
    "\n",
    "It is built from three core pieces:\n",
    "\n",
    "- Value Function (Neural Network) to estimate the optimal cost-to-go for any given state.\n",
    "- Policy (Neural Network) to determine the action to take at a given state.\n",
    "- Monte Carlo Tree Search to simulate and search for the best plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{figure} _static/images/50_alphazero_online_play.png\n",
    ":width: 60%\n",
    ":align: center\n",
    "Illustration of an on-line player such as the one used in AlphaGo,\n",
    "AlphaZero, and Tesauro’s backgammon program. At a given position,\n",
    "it generates a lookahead tree of multiple moves up to some depth, then runs\n",
    "the off-line obtained player for some more moves, and evaluates the effect of the\n",
    "remaining moves by using the position evaluator of the off-line player.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In AlphaZero, the policy and value networks are trained off-line and an approximate version of the fundamental DP algorithm of policy iteration. A separate on-line player is used to select moves, based on multistep lookahead minimization and a terminal position evaluator that was trained using experience with the off-line player.\n",
    "\n",
    "This approach performs better than using the off-line policy directly because of the long lookahead minimization, which corrects for the inevitable imperfections of the neural network-trained off-line\n",
    "player, and position evaluator/terminal cost approximation.\n",
    "\n",
    "In model predictive control (MPC), there is no off-line training and we use the system's model for the on-line rollout. The control interval is equivalent to the number of steps in lookahead minimization, while the prediction interval is equivalent to the total number of steps in lookahead minimization and truncated rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "Monte Carlo Tree Search (MCTS) is a heuristic search algorithm which uses stochastic simulations for decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree.\n",
    "\n",
    "MCTS was combined with neural networks in 2016 and has been used in multiple board games like Chess, Shogi, Checkers, Backgammon, Contract Bridge, Go, Scrabble, and Clobber as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI). \n",
    "\n",
    "The method especially took off due its effectiveness in computer Go and is still being used in DeepMind’s AlphaGoZero the most advanced Go AI to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MCTS consists of 4 steps that are repeated until some condition is met:\n",
    "\n",
    "1. **Selection**: The selection phase traverses the tree level by level, each time\n",
    "   selecting a node based on stored statistics like \"number of visits\"\n",
    "   or \"total reward\". The rule by which the algorithm selects is called the tree policy.\n",
    "   Selection stops when a node is reached that is not fully explored yet, i.e.\n",
    "   not all possible moves have been expanded to new nodes yet.\n",
    "2. **Expansion**: The expansion step consists of adding one or multiple new \n",
    "   child nodes to the final selected node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. **Rollout**: A rollout is initiated from each of these added nodes.\n",
    "   Depending on the mean depth of the tree, rollout is done until the end of the game or\n",
    "   as deep as possible before performing an evaluation. When dealing with\n",
    "   terminal states in games the evaluation is done based on the win condition\n",
    "   and usually yields 0 (loss), 0.5 (draw) or 1 (win).\n",
    "   In the original algorithm the rollout is performed at random, but in general\n",
    "   the so-called default policy can be enhanced by heuristics. For example,\n",
    "   in the case of AlphaGoZero the rollout step was completely replaced by a\n",
    "   neural network evaluation function.\n",
    "4. **Backup**: The outcome of the rollout step is backed up to the nodes involved \n",
    "   in the selection phase by updating their respective statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{figure} _static/images/50_monte_carlo_tree_search.svg\n",
    ":width: 80%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Planning plays an important role in control, decision making and reinforcement learning.\n",
    "- Optimal Control is the branch of Control Theory concerned with optimizing a cost function through the use of open-loop planning.\n",
    "- The Linear Quadratic Regulator (LQR) is an optimal feedback control law that optimizes a quadratic cost function for LTI systems.\n",
    "- Model Predictive Control (MPC) is a control scheme that uses planning in a receding horizon to determine the best action to take at each time step.\n",
    "- MPC uses the system's model for the rollout and can approached in many different ways such as iLQR and DDP.\n",
    "- AlphaZero uses off-line learning and on-line planning to tackle different types of games. Unlike MPC it uses neural networks to determine the next best action and to approximate the cost-to-go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    ":::{figure} ./_static/images/aai-institute-cover.png\n",
    ":width: 90%\n",
    ":align: center\n",
    "---\n",
    "name: aai-institute\n",
    "---\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- [<b id=\"slotine_applied_1991\">[Slotine 1991]</b>](#slotine_applied_1991-back) [Applied nonlinear control.](https://www.academia.edu/download/33582713/Applied_Nonlinear_Control_Slotine.pdf) Slotine, Jean-Jacques E., and Weiping Li. Vol. 199, no. 1. Englewood Cliffs, NJ: Prentice hall. 1991.\n",
    "\n",
    "- [<b id=\"grune_nonlinear_2017\">[Lars Nonlinear 2017]</b>](#grune_nonlinear_2017-back) [Nonlinear model predictive control.](https://link.springer.com/chapter/10.1007/978-3-319-46024-6_3) Grüne, Lars, Jürgen Pannek, Lars Grüne, and Jürgen Pannek. Springer International Publishing, 2017.\n",
    "\n",
    "- [<b id=\"biral_notes_2016\">[Biral Notes 2016]</b>](#biral_notes_2016-back) [Notes on numerical methods for solving optimal control problems.](https://www.jstage.jst.go.jp/article/ieejjia/5/2/5_154/_article/-char/ja/) Biral, Francesco, Enrico Bertolazzi, and Paolo Bosetti. IEEJ Journal of Industry Applications 5, no. 2 (2016): 154-166.\n",
    "\n",
    "- [<b id=\"bertsekas_lessons_2022\">[Bertsektas, Dimitri P, 2022]</b>](#bertsekas_lessons_2022-back) [Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control](http://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf) - Dimitri P. Bertsektas. 2022.\n",
    "\n",
    "- [<b id=\"spencer_optimal_2023\">[Spencer et al. 2023]</b>](#spencer_optimal_2023-back) [AA 203: Optimal and Learning-Based Control.](https://stanfordasl.github.io/aa203/sp2223/) Optimal control solution techniques for systems with known and unknown dynamics. 2023.\n",
    "\n",
    "- [<b id=\"tedrake_underactuated_2023\">[Russ Tedrake, 2023]</b>](#tedrake_underactuated_2023-back) [Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation (Course Notes for MIT 6.832)](http://underactuated.mit.edu/index.html) Russ Tedrake. Downloaded on 24.10.2023."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "scenes_data": {
   "active_scene": "Initialization",
   "init_scene": null,
   "scenes": [
    "Initialization"
   ]
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
