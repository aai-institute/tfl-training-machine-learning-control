{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "hide_input": false,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv",
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_ml_control\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    ":::{figure} ./_static/images/aai-institute-cover.png\n",
    ":width: 90%\n",
    ":align: center\n",
    "---\n",
    "name: aai-institute\n",
    "---\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# MPC and AlphaZero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.\n",
    "\n",
    "It is built from three core pieces:\n",
    "\n",
    "- Value Function (Neural Network) to estimate the optimal cost-to-go for any given state.\n",
    "- Policy (Neural Network) to determine the action to take at a given state.\n",
    "- Monte Carlo Tree Search to simulate and search for the best plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{figure} _static/images/50_alphazero_online_play.png\n",
    ":width: 60%\n",
    ":align: center\n",
    "Illustration of an on-line player such as the one used in AlphaGo,\n",
    "AlphaZero, and Tesauro’s backgammon program. At a given position,\n",
    "it generates a lookahead tree of multiple moves up to some depth, then runs\n",
    "the off-line obtained player for some more moves, and evaluates the effect of the\n",
    "remaining moves by using the position evaluator of the off-line player.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In AlphaZero, the policy and value networks are trained off-line and an approximate version of the fundamental DP algorithm of policy iteration. A separate on-line player is used to select moves, based on multistep lookahead minimization and a terminal position evaluator that was trained using experience with the off-line player.\n",
    "\n",
    "This approach performs better than using the off-line policy directly because of the long lookahead minimization, which corrects for the inevitable imperfections of the neural network-trained off-line\n",
    "player, and position evaluator/terminal cost approximation.\n",
    "\n",
    "In model predictive control (MPC), there is no off-line training and we use the system's model for the on-line rollout. The control interval is equivalent to the number of steps in lookahead minimization, while the prediction interval is equivalent to the total number of steps in lookahead minimization and truncated rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "Monte Carlo Tree Search (MCTS) is a heuristic search algorithm which uses stochastic simulations for decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree.\n",
    "\n",
    "MCTS was combined with neural networks in 2016 and has been used in multiple board games like Chess, Shogi, Checkers, Backgammon, Contract Bridge, Go, Scrabble, and Clobber as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI). \n",
    "\n",
    "The method especially took off due its effectiveness in computer Go and is still being used in DeepMind’s AlphaGoZero the most advanced Go AI to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MCTS consists of 4 steps that are repeated until some condition is met:\n",
    "\n",
    "1. **Selection**: The selection phase traverses the tree level by level, each time\n",
    "   selecting a node based on stored statistics like \"number of visits\"\n",
    "   or \"total reward\". The rule by which the algorithm selects is called the tree policy.\n",
    "   Selection stops when a node is reached that is not fully explored yet, i.e.\n",
    "   not all possible moves have been expanded to new nodes yet.\n",
    "2. **Expansion**: The expansion step consists of adding one or multiple new \n",
    "   child nodes to the final selected node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. **Rollout**: A rollout is initiated from each of these added nodes.\n",
    "   Depending on the mean depth of the tree, rollout is done until the end of the game or\n",
    "   as deep as possible before performing an evaluation. When dealing with\n",
    "   terminal states in games the evaluation is done based on the win condition\n",
    "   and usually yields 0 (loss), 0.5 (draw) or 1 (win).\n",
    "   In the original algorithm the rollout is performed at random, but in general\n",
    "   the so-called default policy can be enhanced by heuristics. For example,\n",
    "   in the case of AlphaGoZero the rollout step was completely replaced by a\n",
    "   neural network evaluation function.\n",
    "4. **Backup**: The outcome of the rollout step is backed up to the nodes involved \n",
    "   in the selection phase by updating their respective statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{figure} _static/images/50_monte_carlo_tree_search.svg\n",
    ":width: 80%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Planning plays an important role in control, decision making and reinforcement learning.\n",
    "- Optimal Control is the branch of Control Theory concerned with optimizing a cost function through the use of open-loop planning.\n",
    "- The Linear Quadratic Regulator (LQR) is an optimal feedback control law that optimizes a quadratic cost function for LTI systems.\n",
    "- Model Predictive Control (MPC) is a control scheme that uses planning in a receding horizon to determine the best action to take at each time step.\n",
    "- MPC uses the system's model for the rollout and can approached in many different ways such as iLQR and DDP.\n",
    "- AlphaZero uses off-line learning and on-line planning to tackle different types of games. Unlike MPC it uses neural networks to determine the next best action and to approximate the cost-to-go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    ":::{figure} ./_static/images/aai-institute-cover.png\n",
    ":width: 90%\n",
    ":align: center\n",
    "---\n",
    "name: aai-institute\n",
    "---\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- [<b id=\"slotine_applied_1991\">[Slotine 1991]</b>](#slotine_applied_1991-back) [Applied nonlinear control.](https://www.academia.edu/download/33582713/Applied_Nonlinear_Control_Slotine.pdf) Slotine, Jean-Jacques E., and Weiping Li. Vol. 199, no. 1. Englewood Cliffs, NJ: Prentice hall. 1991.\n",
    "\n",
    "- [<b id=\"grune_nonlinear_2017\">[Lars Nonlinear 2017]</b>](#grune_nonlinear_2017-back) [Nonlinear model predictive control.](https://link.springer.com/chapter/10.1007/978-3-319-46024-6_3) Grüne, Lars, Jürgen Pannek, Lars Grüne, and Jürgen Pannek. Springer International Publishing, 2017.\n",
    "\n",
    "- [<b id=\"biral_notes_2016\">[Biral Notes 2016]</b>](#biral_notes_2016-back) [Notes on numerical methods for solving optimal control problems.](https://www.jstage.jst.go.jp/article/ieejjia/5/2/5_154/_article/-char/ja/) Biral, Francesco, Enrico Bertolazzi, and Paolo Bosetti. IEEJ Journal of Industry Applications 5, no. 2 (2016): 154-166.\n",
    "\n",
    "- [<b id=\"bertsekas_lessons_2022\">[Bertsektas, Dimitri P, 2022]</b>](#bertsekas_lessons_2022-back) [Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control](http://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf) - Dimitri P. Bertsektas. 2022.\n",
    "\n",
    "- [<b id=\"spencer_optimal_2023\">[Spencer et al. 2023]</b>](#spencer_optimal_2023-back) [AA 203: Optimal and Learning-Based Control.](https://stanfordasl.github.io/aa203/sp2223/) Optimal control solution techniques for systems with known and unknown dynamics. 2023.\n",
    "\n",
    "- [<b id=\"tedrake_underactuated_2023\">[Russ Tedrake, 2023]</b>](#tedrake_underactuated_2023-back) [Underactuated Robotics: Algorithms for Walking, Running, Swimming, Flying, and Manipulation (Course Notes for MIT 6.832)](http://underactuated.mit.edu/index.html) Russ Tedrake. Downloaded on 24.10.2023."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "scenes_data": {
   "active_scene": "Initialization",
   "init_scene": null,
   "scenes": [
    "Initialization"
   ]
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
