{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "hide_input": false,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv",
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext training_ml_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "init_cell": true,
    "scene__Initialization": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "ActiveScene"
    ]
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    ":::{figure} ./_static/images/aai-institute-cover.png\n",
    ":width: 90%\n",
    ":align: center\n",
    "---\n",
    "name: aai-institute\n",
    "---\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# MPC and AlphaZero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.\n",
    "\n",
    "It is built from three core pieces:\n",
    "\n",
    "- Value Function (Neural Network) to estimate the optimal cost-to-go for any given state.\n",
    "- Policy (Neural Network) to determine the action to take at a given state.\n",
    "- Monte Carlo Tree Search (MCTS) to simulate and search for the best plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{figure} _static/images/50_alphazero_online_play.png\n",
    ":width: 60%\n",
    ":align: center\n",
    "Illustration of an on-line player such as the one used in AlphaGo,\n",
    "AlphaZero, and Tesauro’s backgammon program. At a given position,\n",
    "it generates a lookahead tree of multiple moves up to some depth, then runs\n",
    "the off-line obtained player for some more moves, and evaluates the effect of the\n",
    "remaining moves by using the position evaluator of the off-line player.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In AlphaZero, the policy and value networks are trained off-line and an approximate version of the fundamental DP algorithm of policy iteration. A separate on-line player is used to select moves, based on multistep lookahead minimization and a terminal position evaluator that was trained using experience with the off-line player.\n",
    "\n",
    "This approach performs better than using the off-line policy directly because of the long lookahead minimization, which corrects for the inevitable imperfections of the neural network-trained off-line\n",
    "player, and position evaluator/terminal cost approximation.\n",
    "\n",
    "In model predictive control (MPC), there is no off-line training and we use the system's model for the on-line rollout. The control interval is equivalent to the number of steps in lookahead minimization, while the prediction interval is equivalent to the total number of steps in lookahead minimization and truncated rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "Monte Carlo Tree Search (MCTS) is a heuristic search algorithm which uses stochastic simulations for decision processes, most notably those employed in software that plays board games. In that context MCTS is used to solve the game tree.\n",
    "\n",
    "MCTS was combined with neural networks in 2016 and has been used in multiple board games like Chess, Shogi, Checkers, Backgammon, Contract Bridge, Go, Scrabble, and Clobber as well as in turn-based-strategy video games (such as Total War: Rome II's implementation in the high level campaign AI). \n",
    "\n",
    "The method especially took off due its effectiveness in computer Go and is still being used in DeepMind’s AlphaGoZero the most advanced Go AI to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MCTS consists of 4 steps that are repeated until some condition is met:\n",
    "\n",
    "1. **Selection**: The selection phase traverses the tree level by level, each time\n",
    "   selecting a node based on stored statistics like \"number of visits\"\n",
    "   or \"total reward\". The rule by which the algorithm selects is called the tree policy.\n",
    "   Selection stops when a node is reached that is not fully explored yet, i.e.\n",
    "   not all possible moves have been expanded to new nodes yet.\n",
    "2. **Expansion**: The expansion step consists of adding one or multiple new \n",
    "   child nodes to the final selected node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. **Rollout**: A rollout is initiated from each of these added nodes.\n",
    "   Depending on the mean depth of the tree, rollout is done until the end of the game or\n",
    "   as deep as possible before performing an evaluation. When dealing with\n",
    "   terminal states in games the evaluation is done based on the win condition\n",
    "   and usually yields 0 (loss), 0.5 (draw) or 1 (win).\n",
    "   In the original algorithm the rollout is performed at random, but in general\n",
    "   the so-called default policy can be enhanced by heuristics. For example,\n",
    "   in the case of AlphaGoZero the rollout step was completely replaced by a\n",
    "   neural network evaluation function.\n",
    "4. **Backup**: The outcome of the rollout step is backed up to the nodes involved \n",
    "   in the selection phase by updating their respective statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{figure} _static/images/50_monte_carlo_tree_search.svg\n",
    ":width: 80%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{exercise-start} Grid World Again\n",
    ":label: grid-world\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_grid_world_environment(render_mode=\"rgb_array\", max_steps=50)\n",
    "result = simulate_environment(env)\n",
    "show_video(result.frames, fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task can be represented as the following undirected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "G = env.unwrapped.get_graph()\n",
    "plot_grid_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to travel to the goal cell in green. If the cost represents time then we want to find the shortest path to the goal.\n",
    "\n",
    "- Arrows (edges) indicate the possible movements. An action is associated with each edge.\n",
    "- Numbers on edges indicate the cost of moving along an edge.\n",
    "\n",
    "Use Dynamic Programming to solve this problem:\n",
    "\n",
    "- Compute the optimal cost-to-go for each state.\n",
    "- Determine the optimal plan using the computed optimal cost-to-go.\n",
    "- Execute the plan in the environment.\n",
    "\n",
    ":::{tip} Hint 1\n",
    ":class: dropdown\n",
    "Determine all possible paths first.\n",
    "\n",
    "You can use `plot_grid_all_paths_graph(G)`.\n",
    ":::\n",
    "\n",
    ":::{tip} Hint 2\n",
    ":class: dropdown\n",
    "Compute the optimal cost-to-go at each node.\n",
    "\n",
    "You can use `dict(G.nodes(data=True))` to get a dictionary that maps the nodes to their attributes\n",
    "and you can use `G.start_node` and `G.end_node` to access the start and end (i.e. goal) nodes, respectively.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{exercise-end}\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{solution-start} grid-world\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Solution Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{solution} grid-world\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, player):\n",
    "        \"\"\"\n",
    "        Implementation of Monte Carlo Tree Search\n",
    "\n",
    "        Creates a root of an MCTS tree to keep track of the information\n",
    "        obtained throughout the course of the game in the form of a tree\n",
    "        of MCTS nodes\n",
    "\n",
    "        The data structure of a node consists of:\n",
    "          - the game state which it corresponds to\n",
    "          - w, the number of wins that have occurred at or below it in the tree\n",
    "          - n, the number of plays that have occurred at or below it in the tree\n",
    "          - expanded, whether all the children (legal moves) of the node have\n",
    "            been added to the tree\n",
    "\n",
    "        To access the node attributes, use the following format. For example,\n",
    "        to access the attribute 'n' of the root node:\n",
    "          policy = MCTSPolicy()\n",
    "          current_node = policy.root\n",
    "          policy.tree.node[current_node]['n']\n",
    "        \"\"\"\n",
    "        self.digraph = nx.DiGraph()\n",
    "        self.player = player\n",
    "        self.num_simulations = 0\n",
    "        # Constant parameter to weight exploration vs. exploitation for UCT\n",
    "        self.uct_c = np.sqrt(2)\n",
    "\n",
    "        self.node_counter = 0\n",
    "\n",
    "        empty_board = GameState()\n",
    "        self.digraph.add_node(\n",
    "            self.node_counter,\n",
    "            attr_dict={\n",
    "                \"w\": 0,\n",
    "                \"n\": 0,\n",
    "                \"uct\": 0,\n",
    "                \"expanded\": False,\n",
    "                \"state\": empty_board,\n",
    "            },\n",
    "        )\n",
    "        empty_board_node_id = self.node_counter\n",
    "        self.node_counter += 1\n",
    "\n",
    "        self.last_move = None\n",
    "\n",
    "        if player is \"O\":\n",
    "            for successor in [\n",
    "                empty_board.transition_function(*move)\n",
    "                for move in empty_board.legal_moves()\n",
    "            ]:\n",
    "                self.digraph.add_node(\n",
    "                    self.node_counter,\n",
    "                    attr_dict={\n",
    "                        \"w\": 0,\n",
    "                        \"n\": 0,\n",
    "                        \"uct\": 0,\n",
    "                        \"expanded\": False,\n",
    "                        \"state\": successor,\n",
    "                    },\n",
    "                )\n",
    "                self.digraph.add_edge(empty_board_node_id, self.node_counter)\n",
    "                self.node_counter += 1\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.last_move = None\n",
    "\n",
    "    def move(self, starting_state):\n",
    "        # Make a copy of the starting state so that the MCTS state can't be\n",
    "        # modified later from the outside\n",
    "        starting_state = copy.deepcopy(starting_state)\n",
    "        # todo: is that copy needed?\n",
    "\n",
    "        starting_node = None\n",
    "\n",
    "        if self.last_move is not None:\n",
    "            # Check if the starting state is already in the graph as a child of the last move that we made\n",
    "            exists = False\n",
    "            for child in self.digraph.successors(self.last_move):\n",
    "                # Check if the child has the same state attribute as the starting state\n",
    "                if self.digraph.node[child][\"state\"] == starting_state:\n",
    "                    # If it does, then check if there is a link between the last move and this child state\n",
    "                    if self.digraph.has_edge(self.last_move, child):\n",
    "                        exists = True\n",
    "                        starting_node = child\n",
    "            if not exists:\n",
    "                # If it wasn't found, then add the starting state and an edge to it from the last move\n",
    "                self.digraph.add_node(\n",
    "                    self.node_counter,\n",
    "                    attr_dict={\n",
    "                        \"w\": 0,\n",
    "                        \"n\": 0,\n",
    "                        \"uct\": 0,\n",
    "                        \"expanded\": False,\n",
    "                        \"state\": starting_state,\n",
    "                    },\n",
    "                )\n",
    "                self.digraph.add_edge(self.last_move, self.node_counter)\n",
    "                starting_node = self.node_counter\n",
    "                self.node_counter += 1\n",
    "        else:\n",
    "            for node in self.digraph.nodes():\n",
    "                if self.digraph.node[node][\"state\"] == starting_state:\n",
    "                    starting_node = node\n",
    "\n",
    "        computational_budget = 25\n",
    "        for i in range(computational_budget):\n",
    "            self.num_simulations += 1\n",
    "\n",
    "            print(\n",
    "                \"Running MCTS from this starting state with node id {}:\\n{}\".format(\n",
    "                    starting_node, starting_state\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Until computational budget runs out, run simulated trials\n",
    "            # through the tree:\n",
    "\n",
    "            # Selection: Recursively pick the best node that maximizes UCT\n",
    "            # until reaching an unvisited node\n",
    "            print(\"================ ( selection ) ================\")\n",
    "            selected_node = self.selection(starting_node)\n",
    "            print(\"selected:\\n{}\".format(self.digraph.node[selected_node][\"state\"]))\n",
    "\n",
    "            # Check if the selected node is a terminal state, and if so, this\n",
    "            # iteration is finished\n",
    "            if self.digraph.node[selected_node][\"state\"].winner():\n",
    "                break\n",
    "\n",
    "            # Expansion: Add a child node where simulation will start\n",
    "            print(\"================ ( expansion ) ================\")\n",
    "            new_child_node = self.expansion(selected_node)\n",
    "            print(\"Node chosen for expansion:\\n{}\".format(new_child_node))\n",
    "\n",
    "            # Simulation: Conduct a light playout\n",
    "            print(\"================ ( simulation ) ================\")\n",
    "            reward = self.simulation(new_child_node)\n",
    "            print(\"Reward obtained: {}\\n\".format(reward))\n",
    "\n",
    "            # Backpropagation: Update the nodes on the path with the simulation results\n",
    "            print(\"================ ( backpropagation ) ================\")\n",
    "            self.backpropagation(new_child_node, reward)\n",
    "\n",
    "        move, resulting_node = self.best(starting_node)\n",
    "        print(\"MCTS complete. Suggesting move: {}\\n\".format(move))\n",
    "\n",
    "        self.last_move = resulting_node\n",
    "\n",
    "        # If we won, reset the last move to None for future games\n",
    "        if self.digraph.node[resulting_node][\"state\"].winner():\n",
    "            self.last_move = None\n",
    "\n",
    "        return move\n",
    "\n",
    "    def best(self, root):\n",
    "        \"\"\"\n",
    "        Returns the action that results in the child with the highest UCT value\n",
    "        (An alternative strategy could also be used, where the action leading to\n",
    "        the child with the most number of visits is chosen\n",
    "        \"\"\"\n",
    "        # Todo: explore various strategies for choosing the best action\n",
    "        children = self.digraph.successors(root)\n",
    "\n",
    "        # # Option 1: Choose the child with the highest 'n' value\n",
    "        # num_visits = {}\n",
    "        # for child_node in children:\n",
    "        #     num_visits[child_node] = self.digraph.node[child_node]['n']\n",
    "        # best_child = max(num_visits.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "        # Option 2: Choose the child with the highest UCT value\n",
    "        uct_values = {}\n",
    "        for child_node in children:\n",
    "            uct_values[child_node] = self.uct(child_node)\n",
    "\n",
    "        # Choose the child node that maximizes the expected value given by UCT\n",
    "        # If more than one has the same UCT value then break ties randomly\n",
    "        best_children = [\n",
    "            key\n",
    "            for key, val in uct_values.iteritems()\n",
    "            if val == max(uct_values.values())\n",
    "        ]\n",
    "        idx = np.random.randint(len(best_children))\n",
    "        best_child = best_children[idx]\n",
    "\n",
    "        # Determine which action leads to this child\n",
    "        action = self.digraph.get_edge_data(root, best_child)[\"action\"]\n",
    "        return action, best_child\n",
    "\n",
    "    def selection(self, root):\n",
    "        \"\"\"\n",
    "        Starting at root, recursively select the best node that maximizes UCT\n",
    "        until a node is reached that has no explored children\n",
    "        Keeps track of the path traversed by adding each node to path as\n",
    "        it is visited\n",
    "        :return: the node to expand\n",
    "        \"\"\"\n",
    "        # In the case that the root node is not in the graph, add it\n",
    "        if root not in self.digraph.nodes():\n",
    "            self.digraph.add_node(\n",
    "                self.node_counter,\n",
    "                attr_dict={\"w\": 0, \"n\": 0, \"uct\": 0, \"expanded\": False, \"state\": root},\n",
    "            )\n",
    "            self.node_counter += 1\n",
    "            return root\n",
    "        elif not self.digraph.node[root][\"expanded\"]:\n",
    "            print(\"root in digraph but not expanded\")\n",
    "            return root  # This is the node to expand\n",
    "        else:\n",
    "            print(\"root expanded, move on to a child\")\n",
    "            # Handle the general case\n",
    "            children = self.digraph.successors(root)\n",
    "            uct_values = {}\n",
    "            for child_node in children:\n",
    "                uct_values[child_node] = self.uct(state=child_node)\n",
    "\n",
    "            # Choose the child node that maximizes the expected value given by UCT\n",
    "            best_child_node = max(uct_values.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "            return self.selection(best_child_node)\n",
    "\n",
    "    def expansion(self, node):\n",
    "        # As long as this node has at least one unvisited child, choose a legal move\n",
    "        children = self.digraph.successors(node)\n",
    "        legal_moves = self.digraph.node[node][\"state\"].legal_moves()\n",
    "        print(\"Legal moves: {}\".format(legal_moves))\n",
    "\n",
    "        # Select the next unvisited child with uniform probability\n",
    "        unvisited_children = []\n",
    "        corresponding_actions = []\n",
    "        print(\"legal moves: {}\".format(legal_moves))\n",
    "        for move in legal_moves:\n",
    "            print(\"adding to expansion analysis with: {}\".format(move))\n",
    "            child = self.digraph.node[node][\"state\"].transition_function(*move)\n",
    "\n",
    "            in_children = False\n",
    "            for child_node in children:\n",
    "                if self.digraph.node[child_node][\"state\"] == child:\n",
    "                    in_children = True\n",
    "\n",
    "            if not in_children:\n",
    "                unvisited_children.append(child)\n",
    "                corresponding_actions.append(move)\n",
    "        # Todo: why is it possible for there to be no unvisited children?\n",
    "        print(\"unvisited children: {}\".format(len(unvisited_children)))\n",
    "        if len(unvisited_children) > 0:\n",
    "            idx = np.random.randint(len(unvisited_children))\n",
    "            child, move = unvisited_children[idx], corresponding_actions[idx]\n",
    "\n",
    "            self.digraph.add_node(\n",
    "                self.node_counter,\n",
    "                attr_dict={\"w\": 0, \"n\": 0, \"uct\": 0, \"expanded\": False, \"state\": child},\n",
    "            )\n",
    "            self.digraph.add_edge(node, self.node_counter, attr_dict={\"action\": move})\n",
    "            child_node_id = self.node_counter\n",
    "            self.node_counter += 1\n",
    "        else:\n",
    "            # Todo:\n",
    "            # Is this the correct behavior? The issue is, it was getting to the expansion\n",
    "            # expansion method with nodes that were already expanded for an unknown reason,\n",
    "            # so here we return the node that was passed. Maybe there is a case where a\n",
    "            # node had been expanded but not yet marked as expanded until it got here.\n",
    "            return node\n",
    "\n",
    "        # If all legal moves are now children, mark this node as expanded.\n",
    "        if len(children) + 1 == len(legal_moves):\n",
    "            self.digraph.node[node][\"expanded\"] = True\n",
    "            print(\"node is expanded\")\n",
    "\n",
    "        return child_node_id\n",
    "\n",
    "    def simulation(self, node):\n",
    "        \"\"\"\n",
    "        Conducts a light playout from the specified node\n",
    "        :return: The reward obtained once a terminal state is reached\n",
    "        \"\"\"\n",
    "        random_policy = RandomPolicy()\n",
    "        current_state = self.digraph.node[node][\"state\"]\n",
    "        while not current_state.winner():\n",
    "            move = random_policy.move(current_state)\n",
    "            current_state = current_state.transition_function(*move)\n",
    "\n",
    "        if current_state.winner() == self.player:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def backpropagation(self, last_visited, reward):\n",
    "        \"\"\"\n",
    "        Walk the path upwards to the root, incrementing the\n",
    "        'n' and 'w' attributes of the nodes along the way\n",
    "        \"\"\"\n",
    "        current = last_visited\n",
    "        while True:\n",
    "            self.digraph.node[current][\"n\"] += 1\n",
    "            self.digraph.node[current][\"w\"] += reward\n",
    "\n",
    "            print(\n",
    "                \"Updating to n={} and w={}:\\n{}\".format(\n",
    "                    self.digraph.node[current][\"n\"],\n",
    "                    self.digraph.node[current][\"w\"],\n",
    "                    self.digraph.node[current][\"state\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Terminate when we reach the empty board\n",
    "            if self.digraph.node[current][\"state\"] == GameState():\n",
    "                break\n",
    "            # Todo:\n",
    "            # Does this handle the necessary termination conditions for both 'X' and 'O'?\n",
    "            # As far as we can tell, it does\n",
    "\n",
    "            # Will throw an IndexError when we arrive at a node with no predecessors\n",
    "            # Todo: see if this additional check is no longer necessary\n",
    "            try:\n",
    "                current = self.digraph.predecessors(current)[0]\n",
    "            except IndexError:\n",
    "                break\n",
    "\n",
    "    def uct(self, state):\n",
    "        \"\"\"\n",
    "        Returns the expected value of a state, calculated as a weighted sum of\n",
    "        its exploitation value and exploration value\n",
    "        \"\"\"\n",
    "        n = self.digraph.node[state][\"n\"]  # Number of plays from this node\n",
    "        w = self.digraph.node[state][\"w\"]  # Number of wins from this node\n",
    "        t = self.num_simulations\n",
    "        c = self.uct_c\n",
    "        epsilon = EPSILON\n",
    "\n",
    "        exploitation_value = w / (n + epsilon)\n",
    "        exploration_value = c * np.sqrt(np.log(t) / (n + epsilon))\n",
    "        print(\"exploration_value: {}\".format(exploration_value))\n",
    "\n",
    "        value = exploitation_value + exploration_value\n",
    "\n",
    "        print(\"UCT value {:.3f} for state:\\n{}\".format(value, state))\n",
    "\n",
    "        self.digraph.node[state][\"uct\"] = value\n",
    "\n",
    "        return value"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "scenes_data": {
   "active_scene": "Initialization",
   "init_scene": null,
   "scenes": [
    "Initialization"
   ]
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
